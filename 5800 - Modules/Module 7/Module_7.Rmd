---
title: "Module 7"
author: "Dan Miller"
date: "2023-10-20"
output: html_document
---

```{r}
library(sf)
library(tidyverse)

NC <- read_sf("NC_Region/NC_Region.shp")

plot(st_geometry(NC))
NC
```

# Question 1

```{r}

# Create the histogram with density
hist_data <- hist(NC$POP2000, main = "Histogram of POP2000", prob = TRUE, xlab = "POP2000", ylab = "Density", ylim = c(0, 10e-06))

# Creating population mean, median, and density  objects
mean_value <- mean(NC$POP2000)
median_value <- median(NC$POP2000)
density_values <- density(NC$POP2000)

# creating lines based on the above 
abline(v = mean_value, col = "red", lty = 2, lwd = 2)
abline(v = median_value, col = "blue", lty = 2, lwd = 2)
lines(density_values, col = "black", lty = 1, lwd = 2)

# Adding a legend
legend("topright", legend = c(paste("Mean =", round(mean_value, 2)), 
                              paste("Median =", round(median_value, 2)), 
                              "Density"), 
       col = c("red", 
               "blue", 
               "black"), lty = c(2,2,1), lwd = 2)

# Show the histogram

```


### Scatterplots  
**1) Raw Data**

```{r}
library(ggplot2)
ggplot(
data = NC,
mapping = aes(x = MNEM2000, y = POP2000)
) +
geom_point() +
labs(
x = "Manufacturing Jobs",
y = "Population",
# The "\n" character is a "newline" escape character and can be used to
# break up long titles onto more than one line.
title = "Covariance of Manufacturing Jobs vs.\nPopulation in 2000")
```

**2) Transformed Data**
```{r}
ggplot(data = NC, mapping = aes(x = sqrt(MNEM2000), y = sqrt(POP2000))) +
         geom_point()
```

### Testing Covariance Using Pearson's R

```{r}
library(corrr)
cor <- NC %>%
# We'll drop the 'geometry' on the fly, as it can potentially break the
# function. We're not actually getting rid of it from our original data, of
# course.
st_drop_geometry() %>%
  
  dplyr::select(MNEM2000, POP2000) %>%
correlate(use = "pairwise.complete.obs",
method = "pearson")
# The fashion() function in corrr is used to format the results of correlation
# for printing; all it does is make things look nicer in our output!
fashion(cor, decimals = 4)
## term MNEM2000 POP2000
## 1 MNEM2000 .8702
## 2 POP2000 .8702
```
The Pearson’s R for these two variables seems like a strongly positive result – as the
population increases, so does manufacturing employment. We might also like to explore
correlations between numerous variables at the same time; it’s easy to do with the corrr
package and dplyr. Because we’re only looking at numeric data, we can filter out both the
geometry column, as well as any character columns:

```{r}
NC_filter <- NC %>%
  
# Again, we need to drop the geometry
st_drop_geometry() %>%
  
# Using select_if, we can choose only the columns that are numeric
select_if(is.numeric) %>%
  
# Finally, we can choose the columns we actually want to correlate. Note that
# this is a bit redundant with the `select_if()` above, but I wanted to  show
# how to select columns programmatically with a logical test.
dplyr::select(
c(
POP2000,
MNEM2000,
HOUSEHOLDS,
MEDIANRENT,
TOTJOB2000,
WHITE,
BLACK,
AMERI_ES,
ASIAN_PI,
OTHER,
HISPANIC
)
)
```

```{r}
# We can plot a correlogram with the GGally package. Install and load it, and
# use the `ggcorr()` function on our filtered data to get a plot
# install.packages("GGally")
library(GGally)
NC_filter %>%
# We can set a diverging color palette if we set the nbreaks argument. Use
# RColorBrewer::brewer.pal.info to see some of the available color palettes in
# R.
ggcorr(nbreaks = 7, palette = "RdBu")
```

# Question 2

Using the correlate() and fashion() functions in corrr, create a Pearson’s r
correlation matrix of your filtered data. When filtering and selecting columns, choose
different/additional columns to compare (don’t just use the ones in the Lab!).
Provide the matrix and the correlogram (and your R code used to make it!) in your R
Markdown report. Identify the strongest and weakest correlation coefficients where
r < 1.

```{r}
#NC
```

### My Correlation Matrix
```{r}
my_df <- NC %>%
  st_drop_geometry() %>%
  select(
         POP90_SQMI,
         TOTJOB1990,
         MEDIAN_VAL,
         MEDIANRENT,
         WHITE,
         BLACK,
         HISPANIC,
         AMERI_ES,
         ASIAN_PI,
         OTHER,
         MARRIED,
         DIVORCED,
         NEVERMARRY
         
         
         )
  
 # Using `correlate`, with pearson method to create a correlation matrix object 
correlation_matrix <- correlate(my_df, method = "pearson")

# Calling fashion()  to print out correlation matrix
fashion(correlation_matrix)
```

### My Correlogram 
```{r}
## Taking my df and creating a correlogram 
my_df %>%
# We can set a diverging color palette if we set the nbreaks argument. Use
# RColorBrewer::brewer.pal.info to see some of the available color palettes in
# R.
ggcorr(nbreaks = 6, palette = "RdBu")
```


<mark>
**Highest Correlation:** 


**0.99 Correlation:** 

WHITE & TOTJOB1990,  
TOTJOB1990 & NEVERMARRY,   
WHITE & MARRIED,  
WHITE & DIVORCED. 

<mark>
**Lowest Correlation:**  

Everything in the AMERI_ES has low correlation: **(-.09 to 0.14)**  
MEDIAN_VAL & OTHER: **0.23**  
MEDIAN_VAL & HISPANIC: **0.33**





### Regression

```{r}
model <- lm(MNEM2000 ~ POP2000, data = NC)
model
##
## Call:
## lm(formula = MNEM2000 ~ POP2000, data = NC)
##
## Coefficients:
## (Intercept) POP2000
## 1.891e+03 7.294e-02
# What we get is the classic equation of a line: 𝑦 = 𝑚𝑥 + 𝑏, where 𝑏 is our (Intercept) and
# 𝑚 is our POP2000 coefficient. We can plot this in R on top of a scatter plot:
model <- lm(MNEM2000 ~ POP2000, data = NC)
plot(MNEM2000 ~ POP2000, data = NC)
abline(model)
```
```{r}
library(ggplot2)
ggplot(NC, aes(x = POP2000, y = MNEM2000)) +
geom_point() +
# `se = FALSE` means we don't plot a confidence interval around our line.
stat_smooth(method = "lm", se = FALSE)
```

```{r}
s <- summary(model)
s
```
```{r}
s$r.squared
```

Larger 𝑟 values for a bivariate regression line indicate that more of the variance in y is 2
explained by the variance in x. In other words, this means that roughly 76% of
manufacturing in each county is explained by the population of that county; this also means
that ≈ 24% of variance is not explained by population. There’s clearly more to the story,
such as how manufacturing is distributed geographically around the state in a certain way,
but we’ve now established that population is an important part of our model.

### Resdiuals 

```{r}
model <- lm(MNEM2000 ~ POP2000, data = NC)
head(model$residuals)
```

# Question 3 

Calculate and report the mean value of the set of residuals from your regression
results (in other words, the mean difference between predicted and observed values
of our dependent variable). Next, create an absolute value histogram of the residuals
from your regression using the ‘fd’ breaks method. Add the mean value of the
residuals to your plot as a vertical line. Include a legend, appropriate axis labels, and
a title. As always, provide your R code to do this in your R Markdown report. We now have two “goodness of fit” measures to help us decide if our prediction line fits our
data well. A higher r^2 value in combination with a lower standard error estimate gives us some confidence that our line of fit is suitable.

```{r}
# Creating residuals and mean_residual objects
residuals <- model$residuals
mean_residual <- mean(residuals)

# Rounding mean_residual to 1 decimal place for display purposes
formatted_x <-  sprintf("%.1e", mean_residual)

# creating histogram 
hist_abs_residuals <- hist(abs(residuals), 
                           breaks = "fd", 
                           main = "Absolute Value Histogram of Residuals",
                           xlab = "Absolute Residuals", 
                           ylab = "Frequency",
                           col = "lightblue")

# Adding a vertical line for the mean value of residuals
abline(v = mean_residual, col = "red", lwd = 2)

# Adding a legend
legend("topright", 
       legend = paste("Mean Residual = ", formatted_x),  
       
       # Coloring mean_residual line
       col = "red", lwd = 2)

# Adding a title
title("Absolute Value Histogram of Residuals")



```

### Geographic Distribution

```{r}
NC$Residuals <- model$residuals
write_sf(NC, "NC.shp")
```


# Question 4

Create a choropleth map of the residuals of the regression with an appropriate
legend and title. You may explore the spatial patterning fo residuals through altering
your break methods and number of categories, but for your final map, use 4 classes
and a quartiles classification theme. Why? This utilizes the median and 1st/3rd
quartiles as breakpoints. In other words, our map will be connected to a measure of
central tendency of the residuals, which can aid interpretation. Explain in writing if
there is visual evidence for spatial dependence in the map. Provide the map and
your R code.

```{r}
NC
```

```{r}
library(tmap)
library(sp)


### The following code has been commented out. It works, but is ugly. Couldnt figure out how to change the color, but leaving here to refer back to later

## Creating sf (spatial) object from NC df
#sf_object_NC <- st_as_sf(NC)

## passing the spatial object through the qtm() function and filling/coloring with residuals
#qtm(shp = sf_object_NC, fill = "Residuals") 
```

```{r}
# I liked this map better
tm_shape(NC) +
  tm_polygons(
    
    # Data variable being filled for chloropleth
    col = "Residuals",
    
    # Title / Color palette
    title = "Residuals",
    palette = "RdBu",
    
    # Number of classes
    n = 4,
    
    # Method of creating classes (quantile generally preferred)
    style = "quantile") +
  
  # Adding Scale Bar, Default position is "right","bottom"
   tm_scale_bar() +
  
  # Adding compass rose
  tm_compass(position = c("right", "top"), type = "rose", size = 2) +
  
  # Final formatting adjustments
  tm_layout(legend.outside = FALSE,
          title = "NC Residuals",
          inner.margins = .125,
          legend.frame = TRUE
          ) 
```

**Answer**

I really am not completely sure what the question means by spatial dependence. In aggregate it looks like the high and low values approximately balance one another out, which comports with the plot/ histogram in Question 3, i.e., the mean residual is essentially zero. 
However, if we are looking at clustering, and that is what is being referred to as spatial dependence - which makes the most sense to me, then yeah there's a well defined pattern of clustering. High values are close to high values, low values are close to low values, and values near the mean are close to other values near the mean. 
This provides a visualization of where observed values miss above or below our regression line and to what relative degree they miss.  



